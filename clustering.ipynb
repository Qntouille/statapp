{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5444/2277634377.py:11: DtypeWarning: Columns (1,1687,1688) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_dossier_complet_brut = pd.read_csv(file)\n",
      "/tmp/ipykernel_5444/2277634377.py:16: DtypeWarning: Columns (4,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_meta_dossier_complet_brut = pd.read_csv(file)\n",
      "/tmp/ipykernel_5444/2277634377.py:21: DtypeWarning: Columns (1,2,3,4,14,15,16,18,22,24,27,30,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_mobpro_brut = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la matrice : (34881, 34078)\n",
      "DCLT     01001  01002  01004  01005  01006  01007  01008  01009  01010  01011  \\\n",
      "COMMUNE                                                                         \n",
      "01001       11      0      2      0      0      0      0      0      0      0   \n",
      "01002        0      4      2      0      0      2      0      0      0      0   \n",
      "01004        0      0    894      0      0      7      2      0      0      0   \n",
      "01005        0      0      0     31      0      0      0      0      0      0   \n",
      "01006        0      0      0      0      1      0      0      0      0      0   \n",
      "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "97420        0      0      0      0      0      0      0      0      0      0   \n",
      "97421        0      0      0      0      0      0      0      0      0      0   \n",
      "97422        0      0      0      0      0      0      0      0      0      0   \n",
      "97423        0      0      0      0      0      0      0      0      0      0   \n",
      "97424        0      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "DCLT     ...  98803  98805  98807  98809  98811  98814  98815  98818  98822  \\\n",
      "COMMUNE  ...                                                                  \n",
      "01001    ...      0      0      0      0      0      0      0      0      0   \n",
      "01002    ...      0      0      0      0      0      0      0      0      0   \n",
      "01004    ...      0      0      0      0      0      0      0      0      0   \n",
      "01005    ...      0      0      0      0      0      0      0      0      0   \n",
      "01006    ...      0      0      0      0      0      0      0      0      0   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "97420    ...      0      0      0      0      0      0      0      0      0   \n",
      "97421    ...      0      0      0      0      0      0      0      0      0   \n",
      "97422    ...      0      0      0      0      0      0      0      0      0   \n",
      "97423    ...      0      0      0      0      0      0      0      0      0   \n",
      "97424    ...      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "DCLT     99999  \n",
      "COMMUNE         \n",
      "01001        0  \n",
      "01002        0  \n",
      "01004        5  \n",
      "01005        0  \n",
      "01006        0  \n",
      "...        ...  \n",
      "97420        1  \n",
      "97421        0  \n",
      "97422        0  \n",
      "97423        0  \n",
      "97424        0  \n",
      "\n",
      "[34881 rows x 34078 columns]\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "# Information d'accès au cloud MinIO (Bucket de William)\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "MY_BUCKET = \"williamolivier\"\n",
    "# Vérification du contenu\n",
    "fs.ls(f\"{MY_BUCKET}/diffusion\")\n",
    "# Récupération des tableaux\n",
    "FILE_PATH_S3_DCB = f\"{MY_BUCKET}/diffusion/df_dossier_complet_brut.csv\"\n",
    "with fs.open(FILE_PATH_S3_DCB, \"rb\") as file:\n",
    "    df_dossier_complet_brut = pd.read_csv(file)\n",
    "\n",
    "# Récupération des tableaux\n",
    "FILE_PATH_S3_MDCB = f\"{MY_BUCKET}/diffusion/df_meta_dossier_complet_brut.csv\"\n",
    "with fs.open(FILE_PATH_S3_MDCB, \"rb\") as file:\n",
    "    df_meta_dossier_complet_brut = pd.read_csv(file)\n",
    "\n",
    "# Récupération des tableaux\n",
    "FILE_PATH_S3_MB = f\"{MY_BUCKET}/diffusion/df_mobpro_brut.csv\"\n",
    "with fs.open(FILE_PATH_S3_MB, \"rb\") as file:\n",
    "    df_mobpro_brut = pd.read_csv(file)\n",
    "\n",
    "# Récupération des tableaux\n",
    "FILE_PATH_S3_MMB = f\"{MY_BUCKET}/diffusion/df_meta_mobpro_brut.csv\"\n",
    "with fs.open(FILE_PATH_S3_MMB, \"rb\") as file:\n",
    "    df_meta_mobpro_brut = pd.read_csv(file)\n",
    "# Homogénéisation des codes insee (tous en chaîne de caractère)\n",
    "df_mobpro_brut[\"COMMUNE\"] = df_mobpro_brut[\"COMMUNE\"].astype(str).str.zfill(5)\n",
    "df_mobpro_brut[\"DCLT\"] = df_mobpro_brut[\"DCLT\"].astype(str).str.zfill(5)\n",
    "df_mobpro_brut[\"ARM\"] = df_mobpro_brut[\"ARM\"].astype(str).str.zfill(5)\n",
    "# Conversion des colonnes en numérique\n",
    "df_mobpro_brut['NPERR'] = pd.to_numeric(df_mobpro_brut['NPERR'], errors='coerce')\n",
    "df_mobpro_brut['INPSM'] = pd.to_numeric(df_mobpro_brut['INPSM'], errors='coerce')\n",
    "df_mobpro_brut['INPOM'] = pd.to_numeric(df_mobpro_brut['INPOM'], errors='coerce')\n",
    "df_mobpro_brut['INEEM'] = pd.to_numeric(df_mobpro_brut['INEEM'], errors='coerce')\n",
    "\n",
    "# Création des agrégations de base\n",
    "mobpro_villes = df_mobpro_brut.groupby('COMMUNE').agg({\n",
    "    'NPERR': 'sum',\n",
    "    'INPSM': 'sum',\n",
    "    'INPOM': 'sum',\n",
    "    'INEEM': 'sum'})\n",
    "\n",
    "# Créons d'abord un DataFrame temporaire pour chaque valeur d'ILT\n",
    "for i in range(1, 8):\n",
    "    # Grouper par COMMUNE et compter les occurrences où ILT == str(i)\n",
    "    temp_count = df_mobpro_brut[df_mobpro_brut['ILT'] == i].groupby('COMMUNE').size()\n",
    "    # Ajouter cette série au DataFrame principal\n",
    "    mobpro_villes[f'ILT_{i}'] = temp_count\n",
    "\n",
    "# Remplir les valeurs manquantes (NaN) par 0\n",
    "mobpro_villes = mobpro_villes.fillna(0)\n",
    "\n",
    "# Réinitialisation de l'index\n",
    "mobpro_villes = mobpro_villes.reset_index()\n",
    "\n",
    "# On vérifie qu'il y a autant de lignes avec un arrondissement indiqué et de lignes dont la ville de résidence est soit Paris, soit Marseille, soit Lyon (cohérence entre \"ARM\" et \"COMMUNE\")\n",
    "assert len(df_mobpro_brut[df_mobpro_brut['ARM']!=\"ZZZZZ\"])== len(df_mobpro_brut[\n",
    "    (df_mobpro_brut['COMMUNE'] == '75056') | \n",
    "    (df_mobpro_brut['COMMUNE'] == '13055') | \n",
    "    (df_mobpro_brut['COMMUNE'] == '69123')])\n",
    "\n",
    "# Liste des codes insee d'arrondissements\n",
    "arr_paris = [f\"751{str(i).zfill(2)}\" for i in range(1, 21)]\n",
    "arr_marseille = [f\"132{str(i).zfill(2)}\" for i in range(1, 17)]\n",
    "arr_lyon = [f\"6938{str(i).zfill(1)}\" for i in range(1, 10)]\n",
    "df_mobpro_brut.loc[\n",
    "    df_mobpro_brut['COMMUNE'].isin(['75056', '13055', '69123']), 'COMMUNE'\n",
    "] = df_mobpro_brut['ARM']\n",
    "\n",
    "# Création d'une table croisée dynamique pour compter les déplacements\n",
    "flux_tot = pd.crosstab(df_mobpro_brut['COMMUNE'], df_mobpro_brut['DCLT'])\n",
    "\n",
    "# Conversion en DataFrame pour plus de clarté\n",
    "flux_tot = pd.DataFrame(flux_tot)\n",
    "\n",
    "# En option, si on veut réinitialiser l'index pour avoir COMMUNE en colonne :\n",
    "# flux_tot = flux_tot.reset_index()\n",
    "\n",
    "# Pour voir les dimensions de la matrice\n",
    "print(\"Dimensions de la matrice :\", flux_tot.shape)\n",
    "\n",
    "\n",
    "# Filtrer les données pour chaque groupe de transport\n",
    "df_trans_45 = df_mobpro_brut[df_mobpro_brut['TRANS'].isin([4, 5])]\n",
    "df_trans_6 = df_mobpro_brut[df_mobpro_brut['TRANS'] == 6]\n",
    "df_trans_123 = df_mobpro_brut[df_mobpro_brut['TRANS'].isin([1, 2, 3])]\n",
    "\n",
    "# Créer les matrices des flux pour chaque groupe\n",
    "flux_rouge = pd.crosstab(df_trans_45['COMMUNE'], df_trans_45['DCLT'])\n",
    "flux_jaune = pd.crosstab(df_trans_6['COMMUNE'], df_trans_6['DCLT'])\n",
    "flux_vert = pd.crosstab(df_trans_123['COMMUNE'], df_trans_123['DCLT'])\n",
    "\n",
    "# Convertir en DataFrame pour plus de clarté\n",
    "flux_rouge = pd.DataFrame(flux_rouge)\n",
    "flux_jaune = pd.DataFrame(flux_jaune)\n",
    "flux_vert = pd.DataFrame(flux_vert)\n",
    "\n",
    "def flux(ville_a,ville_b):\n",
    "    flux = flux_tot.loc[ville_a, ville_b]\n",
    "    print(f\"Nombre de personnes se déplaçant de {ville_a} vers {ville_b} : {flux}\")\n",
    "\n",
    "## Prend en arg deux codes insee en chaîne de caractère et renvoie le nombre de commute entre ces deux villes\n",
    "\n",
    "# Top 10 des villes générant le plus de déplacements\n",
    "top_villes_depart = flux_tot.sum(axis=1).sort_values(ascending=False).head(10)\n",
    "top_villes_arrivee = flux_tot.sum(axis=0).sort_values(ascending=False).head(10)\n",
    "\n",
    "transport_dict = {\n",
    "    1: \"Pas de transport\",\n",
    "    2: \"Marche à pied (ou rollers, patinette)\",\n",
    "    3: \"Vélo (y compris à assistance électrique)\",\n",
    "    4: \"Deux-roues motorisé\",\n",
    "    5: \"Voiture, camion, fourgonnette\",\n",
    "    6: \"Transports en commun\"\n",
    "}\n",
    "print(flux_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "flux_tot_scaled = scaler.fit_transform(flux_tot)  # Normaliser flux_tot\n",
    "\n",
    "# Application du modèle Agglomerative Clustering (avec 50 clusters)\n",
    "agglomerative = AgglomerativeClustering(n_clusters=50)\n",
    "labels_agglomerative = agglomerative.fit_predict(flux_tot_scaled)  # Appliquer sur flux_tot normalisé\n",
    "\n",
    "# Ajout des labels des clusters au DataFrame original (flux_tot) pour les visualiser\n",
    "flux_tot['Cluster'] = labels_agglomerative\n",
    "\n",
    "# Affichage des résultats sous forme de DataFrame avec les villes et leurs clusters\n",
    "clustered_data = flux_tot.copy()\n",
    "clustered_data['Cluster'] = labels_agglomerative\n",
    "\n",
    "# Visualisation des résultats d'Agglomerative Clustering\n",
    "# Ici, je suppose que les indices des villes sont dans le DataFrame (index ou une colonne nommée 'Ville')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(clustered_data.index, np.zeros_like(clustered_data.index), c=labels_agglomerative, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering des Villes\")\n",
    "plt.xlabel(\"Villes\")\n",
    "plt.ylabel(\"Clusters\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
